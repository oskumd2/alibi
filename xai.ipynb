{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install alibi[torch]\n",
    "import alibi\n",
    "\n",
    "from alibi.explainers import CounterfactualRL\n",
    "from alibi.models.pytorch import AE\n",
    "from alibi.models.pytorch import Actor, Critic\n",
    "#from alibi.models.pytorch import MNISTEncoder, MNISTDecoder, MNISTClassifier\n",
    "from alibi.explainers.cfrl_base import Callback\n",
    "from alibi.models.pytorch.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JHA (240403): changed the Alibi MNISTEncoder, MNISTDEcoder, MNISTClassifier from 1 channel (gray-scale) to 3 channels (RGB)\n",
    "\n",
    "class MNISTClassifier(Model):\n",
    "    \"\"\"\n",
    "    MNIST classifier used in the experiments for Counterfactual with Reinforcement Learning. The model consists of two\n",
    "    convolutional layers having 64 and 32 channels and a kernel size of 2 with ReLU nonlinearities, followed by\n",
    "    maxpooling of size 2 and dropout of 0.3. The convolutional block is followed by a fully connected layer of 256 with\n",
    "    ReLU nonlinearity, and finally a fully connected layer is used to predict the class logits (10 in MNIST case).\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_dim\n",
    "            Output dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(2, 2), padding=1) \n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=(2, 2), padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1568, 256)\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "\n",
    "        # send to device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            Input tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Classification logits.\n",
    "        \"\"\"\n",
    "        x = self.dropout1(self.maxpool1(F.relu(self.conv1(x))))\n",
    "        x = self.dropout2(self.maxpool2(F.relu(self.conv2(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MNISTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    MNIST encoder used in the experiments for the Counterfactual with Reinforcement Learning. The model\n",
    "    consists of 3 convolutional layers having 16, 8 and 8 channels and a kernel size of 3, with ReLU nonlinearities.\n",
    "    Each convolutional layer is followed by a maxpooling layer of size 2. Finally, a fully connected layer\n",
    "    follows the convolutional block with a tanh nonlinearity. The tanh clips the output between [-1, 1], required\n",
    "    in the DDPG algorithm (e.g., [act_low, act_high]). The embedding dimension used in the paper is 32, although\n",
    "    this can vary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_dim\n",
    "            Latent dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=(3, 3), padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d((2, 2), stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=(3, 3), padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d((2, 2), stride=2)\n",
    "        self.conv3 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding=2)\n",
    "        self.maxpool3 = nn.MaxPool2d((2, 2), stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(8 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            Input tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Encoding representation having each component in the interval [-1, 1]\n",
    "        \"\"\"\n",
    "        x = self.maxpool1(F.relu(self.conv1(x)))\n",
    "        x = self.maxpool2(F.relu(self.conv2(x)))\n",
    "        x = self.maxpool3(F.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "class MNISTDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    MNIST decoder used in the Counterfactual with Reinforcement Learning experiments. The model consists of a fully\n",
    "    connected layer of 128 units with ReLU activation followed by a convolutional block. The convolutional block\n",
    "    consists fo 4 convolutional layers having 8, 8, 8  and 1 channels and a kernel size of 3. Each convolutional layer,\n",
    "    except the last one, has ReLU nonlinearities and is followed by an upsampling layer of size 2. The final layers\n",
    "    uses a sigmoid activation to clip the output values in [0, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_dim\n",
    "            Latent dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.conv1 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding=1)\n",
    "        self.up1 = nn.Upsample(scale_factor=2)\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding=1)\n",
    "        self.up2 = nn.Upsample(scale_factor=2)\n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size=(3, 3))\n",
    "        self.up3 = nn.Upsample(scale_factor=2)\n",
    "        self.conv4 = nn.Conv2d(16, 3, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            Input tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Decoded input having each component in the interval [0, 1].\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(x.shape[0], 8, 4, 4)\n",
    "        x = self.up1(F.relu(self.conv1(x)))\n",
    "        x = self.up2(F.relu(self.conv2(x)))\n",
    "        x = self.up3(F.relu(self.conv3(x)))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed function\n",
    "def seed_everything(seed = 23):\n",
    "    # tests\n",
    "    assert isinstance(seed, int), 'seed has to be an integer'\n",
    "    \n",
    "    # randomness\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU CHECK\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available. Training on CPU...')\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    print('CUDA is available. Training on GPU...')\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOMNESS\n",
    "seed = 98\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants.\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "# Load MNIST dataset.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define trainset.\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define testset.\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run xai_setting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PREPARATION\n",
    "\n",
    "# load splits\n",
    "data_train = train\n",
    "data_valid = test\n",
    "\n",
    "# create datasets\n",
    "train_dataset = EyeData(data      = data_train, \n",
    "                             directory = './input/diabetic-retinopathy-resized/resized_train/train',   # JHA (240403): original directory is ./resized_train\n",
    "                             transform = train_trans,\n",
    "                             itype ='.jpeg')\n",
    "valid_dataset = EyeData(data       = data_valid, \n",
    "                            directory  = './input/aptos2019-blindness-detection/train_images',\n",
    "                            transform  = valid_trans,\n",
    "                            itype ='.png')\n",
    "\n",
    "# create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size  = batch_size, \n",
    "                                           shuffle     = True, \n",
    "                                           num_workers = 0)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                                           batch_size  = batch_size, \n",
    "                                           shuffle     = False, \n",
    "                                           num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define already-trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b7\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#I/O with regards to .bin files\n",
    "\n",
    "#Loading model for inference\n",
    "model = EfficientNet.from_pretrained('efficientnet-b7', num_classes = 5)\n",
    "model.load_state_dict(torch.load('C:/rsrch/240305_TorchDR/models/model_enet_b7_fold4.bin'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Loading model for training\n",
    "# Load the .bin model\n",
    "model = torch.load('C:/rsrch/240305_TorchDR/models/model_enet_b7_fold4.bin')\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "'''\n",
    "\n",
    "classifier = model = EfficientNet.from_pretrained('efficientnet-b7', num_classes = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the predictor (black-box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictor function (black-box) used to train the CFRL\n",
    "def predictor(X: np.ndarray):\n",
    "    Y = classifier(X).numpy()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define autoencoder trainset.\n",
    "trainset_ae = train_loader\n",
    "\n",
    "# Define autoencode testset.\n",
    "testset_ae = valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x8712 and 128x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# forward and backward pass\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 34\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model(\u001b[43mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     35\u001b[0m     loss  \u001b[38;5;241m=\u001b[39m loss(preds, labels)\n\u001b[0;32m     36\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\oskum\\anaconda3\\envs\\240305\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oskum\\anaconda3\\envs\\240305\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[75], line 8\u001b[0m, in \u001b[0;36mAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 8\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_hat\n",
      "File \u001b[1;32mc:\\Users\\oskum\\anaconda3\\envs\\240305\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oskum\\anaconda3\\envs\\240305\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[71], line 102\u001b[0m, in \u001b[0;36mMNISTEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    100\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool3(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n\u001b[0;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m--> 102\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\oskum\\anaconda3\\envs\\240305\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oskum\\anaconda3\\envs\\240305\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oskum\\anaconda3\\envs\\240305\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x8712 and 128x64)"
     ]
    }
   ],
   "source": [
    "# Define autoencoder path and create dir if it doesn't exist.\n",
    "ae_path = os.path.join(\"pytorch\", \"DR_autoencoder\")\n",
    "if not os.path.exists(ae_path):\n",
    "    os.makedirs(ae_path)\n",
    "   \n",
    "# Define latent dimension.\n",
    "LATENT_DIM = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "# Compile autoencoder.\n",
    "ae = AE(encoder=MNISTEncoder(latent_dim=LATENT_DIM),\n",
    "        decoder=MNISTDecoder(latent_dim=LATENT_DIM))\n",
    "ae.to(device)\n",
    "ae.train()\n",
    "\n",
    "# Define optimizer and loss function.\n",
    "loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "\n",
    "if len(os.listdir(ae_path)) == 0:\n",
    "    # Fit and save autoencoder.\n",
    "    # loop through batches\n",
    "    for batch_i, data in enumerate(train_loader):\n",
    "\n",
    "        # extract inputs and labels\n",
    "        inputs = data['image']\n",
    "        labels = data['label'].view(-1)\n",
    "        inputs = inputs.to(device, dtype = torch.float)\n",
    "        labels = labels.to(device, dtype = torch.long)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward and backward pass\n",
    "        with torch.set_grad_enabled(True):\n",
    "            preds = model(ae(inputs))\n",
    "            loss  = loss(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    torch.save(ae.state_dict(), ae_path)\n",
    "else:\n",
    "    # Load the model.\n",
    "    ae.load_state_dict(torch.load(ae_path))\n",
    "    ae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrraining AE needed\n",
    "explainer = CounterfactualRL(predictor=predictor,\n",
    "                             encoder=ae.encoder,\n",
    "                             decoder=ae.decoder,\n",
    "                             coeff_sparsity=COEFF_SPARSITY,\n",
    "                             coeff_consistency=COEFF_CONSISTENCY,\n",
    "                             latent_dim=LATENT_DIM,\n",
    "                             train_steps=300000,\n",
    "                             batch_size=100,\n",
    "                             backend=\"pytorch\")\n",
    "\n",
    "# calling X_train needed\n",
    "explainer.fit(X=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation step\n",
    "explanation = explainer.explain(X=X_test,\n",
    "                                Y_t=np.array([1]),\n",
    "                                batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "240305",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
